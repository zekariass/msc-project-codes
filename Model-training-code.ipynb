{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f22a058-c169-41de-9248-0ed445e7211a",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- From the three models, run one of the cell with label CUSTOM MODEL, EfficientNetv2 Transfer Model, or InceptionResNet Transfer Model to instantiate the model. Do not rull the three cells together to have a clue which model you are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ee385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6588c1",
   "metadata": {},
   "source": [
    "# BATCH SIZE and SEED\n",
    "\n",
    "Set the batch size and set the seed for randomisation. It allows to have reproducible experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1875c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Seed settings\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac6b17",
   "metadata": {},
   "source": [
    "# SET DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cc11bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Running on device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ea258",
   "metadata": {},
   "source": [
    "# DATASET CLASS FOR HEAD POSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05629fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T19:01:33.197135Z",
     "iopub.status.busy": "2024-07-21T19:01:33.196329Z",
     "iopub.status.idle": "2024-07-21T19:01:33.211463Z",
     "shell.execute_reply": "2024-07-21T19:01:33.210483Z",
     "shell.execute_reply.started": "2024-07-21T19:01:33.197077Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders containing images and annotation files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Traverse through each subfolder in the root directory\n",
    "        for subfolder in os.listdir(self.root_dir):\n",
    "            subfolder_path = os.path.join(self.root_dir, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                # Locate the annotation file in the subfolder\n",
    "                annotation_file = os.path.join(subfolder_path, 'angles.csv')\n",
    "                if os.path.exists(annotation_file):\n",
    "                    annotations = pd.read_csv(annotation_file)\n",
    "                    for idx in range(len(annotations)):\n",
    "                        img_id = str(annotations.iloc[idx, 0])\n",
    "                        img_name = f'frame_{img_id}.png'  # Add .png extension\n",
    "                        img_path = os.path.join(subfolder_path, img_name)\n",
    "                        \n",
    "                        if os.path.exists(img_path):\n",
    "                            self.image_paths.append(img_path)\n",
    "                            yaw = annotations.iloc[idx, 1]\n",
    "                            roll = annotations.iloc[idx, 2]\n",
    "                            pitch = annotations.iloc[idx, 3]\n",
    "                            self.labels.append([yaw, roll, pitch])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    def get_random_sample(self):\n",
    "        # Get a random index\n",
    "        random_idx = random.randint(0, self.__len__() - 1)\n",
    "        return self.__getitem__(random_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3aee9",
   "metadata": {},
   "source": [
    "# DATASET CLASS FOR EYEGAZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0bbc75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T15:08:53.717877Z",
     "iopub.status.busy": "2024-07-21T15:08:53.717105Z",
     "iopub.status.idle": "2024-07-21T15:08:53.729744Z",
     "shell.execute_reply": "2024-07-21T15:08:53.728883Z",
     "shell.execute_reply.started": "2024-07-21T15:08:53.717844Z"
    }
   },
   "outputs": [],
   "source": [
    "# EYE GAZE DATASET\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        # Iterate through level 1 folders\n",
    "        for level1_folder in os.listdir(self.root_dir):\n",
    "            level1_path = os.path.join(self.root_dir, level1_folder)\n",
    "            if os.path.isdir(level1_path):\n",
    "                # Iterate through level 2 folders\n",
    "                for level2_folder in os.listdir(level1_path):\n",
    "                    level2_path = os.path.join(level1_path, level2_folder)\n",
    "                    if os.path.isdir(level2_path):\n",
    "                        # Load the annotations CSV\n",
    "                        csv_path = os.path.join(level2_path, 'lookat_points.csv')\n",
    "                        annotations_df = pd.read_csv(csv_path)\n",
    "                        \n",
    "                        # Gather all image paths and corresponding labels\n",
    "                        for file in os.listdir(level2_path):\n",
    "                            if file.endswith('.png'):\n",
    "                                image_id = int(file.split('.')[0])\n",
    "                                annotation_row = annotations_df[annotations_df['id'] == image_id]\n",
    "                                \n",
    "                                if not annotation_row.empty:\n",
    "                                    x, y = annotation_row.iloc[0]['x'], annotation_row.iloc[0]['y']\n",
    "                                    relative_path = os.path.join(level1_folder, level2_folder, file).replace('\\\\', '/')\n",
    "                                    \n",
    "                                    self.image_paths.append(relative_path)\n",
    "                                    self.labels.append((x, y))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path\n",
    "        img_path = os.path.join(self.root_dir, self.image_paths[idx])\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get the label for this image\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63dd00",
   "metadata": {},
   "source": [
    "# PREPARE DATA\n",
    "Prepare the dataset for training. Comment and uncomment the appropriate transform object below based on the task type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef82cb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T19:01:55.694147Z",
     "iopub.status.busy": "2024-07-21T19:01:55.693202Z",
     "iopub.status.idle": "2024-07-21T19:12:38.396541Z",
     "shell.execute_reply": "2024-07-21T19:12:38.395398Z",
     "shell.execute_reply.started": "2024-07-21T19:01:55.694112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4028\n",
      "1151\n",
      "576\n"
     ]
    }
   ],
   "source": [
    "# Transformation for head pose\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((299, 299)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4702, 0.3964, 0.3711), (0.2337, 0.2362, 0.2483))\n",
    "# ])\n",
    "\n",
    "# Transformation for eye gaze\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.6965, 0.5065, 0.40670), (0.2380, 0.2134, 0.1928))\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = CustomImageDataset(root_dir='/kaggle/input/headposeimgs/all_head_pose', transform=transform)\n",
    "\n",
    "# Define the partition percentages\n",
    "train_percentage = 0.7\n",
    "val_percentage = 0.2\n",
    "test_percentage = 0.1\n",
    "\n",
    "# Calculate the sizes for each partition\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_percentage * dataset_size)\n",
    "val_size = int(val_percentage * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size  # Ensure the sum equals the dataset size\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Split the dataset\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader for each partition\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "# Example usage: Access a random sample from the training set\n",
    "# random_sample = train_dataset[random.randint(0, len(train_dataset)-1)]\n",
    "# print(random_sample['image'].shape, random_sample['labels'])\n",
    "\n",
    "# print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18bdd9c",
   "metadata": {},
   "source": [
    "# CUSTOM MODEL\n",
    "\n",
    "Classes for custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1858d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:00:16.046543Z",
     "iopub.status.busy": "2024-07-21T13:00:16.046186Z",
     "iopub.status.idle": "2024-07-21T13:00:16.054953Z",
     "shell.execute_reply": "2024-07-21T13:00:16.053924Z",
     "shell.execute_reply.started": "2024-07-21T13:00:16.046514Z"
    }
   },
   "outputs": [],
   "source": [
    "class FactorizedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride=1):\n",
    "        super(FactorizedConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size), stride=stride, padding=(0, padding), bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(kernel_size, 1), stride=stride, padding=(padding, 0), bias=True)\n",
    "      \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.ReLU(inplace=True)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return nn.ReLU(inplace=True)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bd9eb7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:00:16.308844Z",
     "iopub.status.busy": "2024-07-21T13:00:16.307994Z",
     "iopub.status.idle": "2024-07-21T13:00:16.322431Z",
     "shell.execute_reply": "2024-07-21T13:00:16.321427Z",
     "shell.execute_reply.started": "2024-07-21T13:00:16.308811Z"
    }
   },
   "outputs": [],
   "source": [
    "class InceptionResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, scale=1.0):\n",
    "        super(InceptionResNetBlock, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            FactorizedConv2d(32, 48, kernel_size=3, padding=1),\n",
    "            FactorizedConv2d(48, 64, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            FactorizedConv2d(32, 48, kernel_size=3, padding=1),\n",
    "            FactorizedConv2d(48, 64, kernel_size=7, padding=3),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            FactorizedConv2d(32, 48, kernel_size=5, padding=2),\n",
    "            FactorizedConv2d(48, 64, kernel_size=7, padding=3),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=True)\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv2d(128, in_channels, kernel_size=1, bias=True)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch0 = self.branch0(x)\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "\n",
    "        mixed = torch.cat([branch0, branch1, branch2, branch3], dim=1)\n",
    "        up = self.conv(mixed)\n",
    "        up = self.bn(up)\n",
    "\n",
    "        x = x + self.scale * up\n",
    "        return nn.ReLU(inplace=True)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8551eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:00:16.916636Z",
     "iopub.status.busy": "2024-07-21T13:00:16.916275Z",
     "iopub.status.idle": "2024-07-21T13:00:16.931894Z",
     "shell.execute_reply": "2024-07-21T13:00:16.930820Z",
     "shell.execute_reply.started": "2024-07-21T13:00:16.916607Z"
    }
   },
   "outputs": [],
   "source": [
    "class InceptionResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(InceptionResNet, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(32, 32, kernel_size=3, padding=0, bias=True),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "            nn.Conv2d(64, 80, kernel_size=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(80),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(80, 192, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.inception_resnet_a = nn.Sequential(\n",
    "            InceptionResNetBlock(192, scale=0.2),\n",
    "            InceptionResNetBlock(192, scale=0.2),\n",
    "            # InceptionResNetBlock(192, scale=0.2),\n",
    "            # InceptionResNetBlock(192, scale=0.2)\n",
    "        )\n",
    "\n",
    "        self.reduction_a = nn.Sequential(\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inception_resnet_b = nn.Sequential(\n",
    "            InceptionResNetBlock(384, scale=0.2),\n",
    "            InceptionResNetBlock(384, scale=0.2),\n",
    "            # InceptionResNetBlock(384, scale=0.2),\n",
    "            # InceptionResNetBlock(384, scale=0.2)\n",
    "        )\n",
    "\n",
    "        self.reduction_b = nn.Sequential(\n",
    "            nn.Conv2d(384, 1024, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inception_resnet_c = nn.Sequential(\n",
    "            InceptionResNetBlock(1024, scale=0.2),\n",
    "            InceptionResNetBlock(1024, scale=0.2),\n",
    "            # InceptionResNetBlock(1024, scale=0.2),\n",
    "            # InceptionResNetBlock(1024, scale=0.2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception_resnet_a(x)\n",
    "        x = self.reduction_a(x)\n",
    "        x = self.inception_resnet_b(x)\n",
    "        x = self.reduction_b(x)\n",
    "        x = self.inception_resnet_c(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6436db-d65a-415a-b9c4-32c1fa48581c",
   "metadata": {},
   "source": [
    "Create the model for the custom model. here, InceptionResNet is the customized model for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ff43044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:00:21.629201Z",
     "iopub.status.busy": "2024-07-21T13:00:21.628590Z",
     "iopub.status.idle": "2024-07-21T13:00:21.754002Z",
     "shell.execute_reply": "2024-07-21T13:00:21.753035Z",
     "shell.execute_reply.started": "2024-07-21T13:00:21.629161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6323762\n"
     ]
    }
   ],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "model = InceptionResNet(num_classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "num_params = count_parameters(model)\n",
    "print(f'Total number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27ac3b",
   "metadata": {},
   "source": [
    "# EfficientNetv2 Transfer Model\n",
    "\n",
    "Load and create the EfficientNetv2 pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eca240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Regression(nn.Module):\n",
    "    def __init__(self, num_outputs=2):\n",
    "        super(EfficientNetV2Regression, self).__init__()\n",
    "        self.model = models.efficientnet_v2_s(weights=None)#weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "       \n",
    "        # Replace the classifier layer to output `num_outputs` regression targets\n",
    "        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, num_outputs)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetV2Regression(num_outputs=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15e05e-8217-4762-b8f2-a380161ecb73",
   "metadata": {},
   "source": [
    "#  InceptionResNet Transfer Model\n",
    "\n",
    "Load and create the InceptionResNet pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9c450-8f9d-424c-89bb-1125c48e68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model = timm.create_model('inception_resnet_v2', pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "in_features = model.classif.in_features\n",
    "model.classif = nn.Linear(in_features, 3)\n",
    "model.classif = model.classif.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1a6e5",
   "metadata": {},
   "source": [
    "Define the Mean Absolute Error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fb63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss functions\n",
    "def l1_loss(outputs, targets):\n",
    "    return nn.L1Loss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ccde4f",
   "metadata": {},
   "source": [
    "# DECLARE STATISTICS VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39d5367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T15:21:39.688851Z",
     "iopub.status.busy": "2024-07-21T15:21:39.688488Z",
     "iopub.status.idle": "2024-07-21T15:21:39.695783Z",
     "shell.execute_reply": "2024-07-21T15:21:39.694881Z",
     "shell.execute_reply.started": "2024-07-21T15:21:39.688823Z"
    }
   },
   "outputs": [],
   "source": [
    "train_stats = {}\n",
    "val_stats = {}\n",
    "checkpoint = {}\n",
    "start_epoch = 0\n",
    "output_path = '/kaggle/working/'\n",
    "resume_training = True\n",
    "\n",
    "if resume_training:\n",
    "    # Path to the stat files\n",
    "    prev_train_stats_file_path = '/kaggle/working/effinet_train_all_stat_dict.pth'\n",
    "    prev_val_stats_file_path = '/kaggle/working/effinet_val_all_stat_dict.pth'\n",
    "    prev_checkpoint_path = '/kaggle/working/effinet_checkpoint_at_epoch_3_loss_3_18682163.pth'\n",
    "\n",
    "    # Read the train stat file\n",
    "    with open(prev_train_stats_file_path, 'rb') as file:\n",
    "        train_stats = torch.load(file)\n",
    "\n",
    "    # Read the val stat file\n",
    "    with open(prev_val_stats_file_path, 'rb') as file:\n",
    "        val_stats = torch.load(file)\n",
    "\n",
    "    print(\"TRAIN STATS: \",len(train_stats))\n",
    "    print(\"VAL STATS: \", len(val_stats))\n",
    "\n",
    "    print(\"RESUMING TRAINING FROM PREVIOUS STOPPED MODEL....\")\n",
    "        \n",
    "#     model.load_state_dict(torch.load(prev_checkpoint_path)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62ad24",
   "metadata": {},
   "source": [
    "# CHECK NUMBER OF GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f417fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T15:21:42.666830Z",
     "iopub.status.busy": "2024-07-21T15:21:42.666189Z",
     "iopub.status.idle": "2024-07-21T15:21:42.672040Z",
     "shell.execute_reply": "2024-07-21T15:21:42.671160Z",
     "shell.execute_reply.started": "2024-07-21T15:21:42.666797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs: {num_gpus}')\n",
    "else:\n",
    "    print('No GPUs available.')\n",
    "\n",
    "# FINE TUNE\n",
    "# checkpoint = torch.load(\"checkpoint_at_epoch_26_loss_1_63385426.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae14aa",
   "metadata": {},
   "source": [
    "# HEAD POSE TRAINING\n",
    "\n",
    "Training code for head pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e36fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T15:22:04.791491Z",
     "iopub.status.busy": "2024-07-21T15:22:04.791127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST VAL LOSS IS:  inf\n",
      "INITIAL LEARNING RATE:  0.001\n",
      "INITIAL TRAIN STATS:  0\n",
      "INITIAL VAL STATS:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 4028/4028 [42:08<00:00,  1.59batch/s, loss=7.79]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Avg Loss: 7.7884, Yaw Loss: 8.8732, Roll Loss: 7.0448, Pitch Loss: 7.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1151/1151 [07:39<00:00,  2.51batch/s, loss=5.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 5.1381, Yaw Loss: 6.4810, Roll Loss: 4.3687, Pitch Loss: 4.5646, RMSE: 8.3053, R2: 0.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 4028/4028 [36:33<00:00,  1.84batch/s, loss=5.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Avg Loss: 5.0797, Yaw Loss: 6.3782, Roll Loss: 4.1660, Pitch Loss: 4.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1151/1151 [04:56<00:00,  3.88batch/s, loss=4.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 4.5727, Yaw Loss: 5.8986, Roll Loss: 3.7424, Pitch Loss: 4.0771, RMSE: 7.3029, R2: 0.8257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 4028/4028 [38:52<00:00,  1.73batch/s, loss=4.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Avg Loss: 4.1971, Yaw Loss: 5.5364, Roll Loss: 3.3240, Pitch Loss: 3.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1151/1151 [05:06<00:00,  3.75batch/s, loss=3.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 3.6536, Yaw Loss: 5.0174, Roll Loss: 2.5220, Pitch Loss: 3.4214, RMSE: 5.9587, R2: 0.8804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50:  44%|████▎     | 1758/4028 [16:01<20:53,  1.81batch/s, loss=1.67] "
     ]
    }
   ],
   "source": [
    "# criterion = nn.L1Loss()  # Mean Absolute Error Loss\n",
    "initial_lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# Define the training loop\n",
    "num_epochs = 50\n",
    "starting_epoch = 0\n",
    "patience = 1\n",
    "if resume_training:\n",
    "    best_val_loss = val_stats[max(val_stats.keys())]['avg_mae']\n",
    "else:\n",
    "    best_val_loss = float('inf')\n",
    "print(\"CURRENT BEST VAL LOSS IS: \", best_val_loss)\n",
    "    \n",
    "early_stop_counter = 0\n",
    "successive_lr_update_count = 0\n",
    "max_successive_lr_update_count = 2\n",
    "\n",
    "best_checkpoint_name = ''\n",
    "lr = initial_lr\n",
    "print(\"INITIAL LEARNING RATE: \", lr)\n",
    "\n",
    "train_all_stat_dict = train_stats\n",
    "val_all_stat_dict = val_stats\n",
    "print(\"INITIAL TRAIN STATS: \", len(train_all_stat_dict))\n",
    "print(\"INITIAL VAL STATS: \", len(val_all_stat_dict))\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs!')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "for epoch in range(starting_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_yaw_loss = 0.0\n",
    "    running_roll_loss = 0.0\n",
    "    running_pitch_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", mininterval=1)\n",
    "\n",
    "    for inputs, targets in train_loader_tqdm:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute L1 loss for yaw, roll, and pitch\n",
    "        yaw_loss = l1_loss(outputs[:, 0], targets[:, 0])\n",
    "        roll_loss = l1_loss(outputs[:, 1], targets[:, 1])\n",
    "        pitch_loss = l1_loss(outputs[:, 2], targets[:, 2])\n",
    "        avg_loss = (yaw_loss + roll_loss + pitch_loss) / 3\n",
    "        \n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += avg_loss.item()\n",
    "        running_yaw_loss += yaw_loss.item()\n",
    "        running_roll_loss += roll_loss.item()\n",
    "        running_pitch_loss += pitch_loss.item()\n",
    "        \n",
    "        train_loader_tqdm.set_postfix(loss=running_loss / (len(train_loader_tqdm) + 1))\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_yaw_loss = running_yaw_loss / num_batches\n",
    "    avg_roll_loss = running_roll_loss / num_batches\n",
    "    avg_pitch_loss = running_pitch_loss / num_batches\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Yaw Loss: {avg_yaw_loss:.4f}, Roll Loss: {avg_roll_loss:.4f}, Pitch Loss: {avg_pitch_loss:.4f}')\n",
    "    \n",
    "\n",
    "    train_all_stat_dict[epoch + 1] = {\n",
    "        'lr': lr,\n",
    "        'mae': avg_loss,\n",
    "        'yaw_mae': avg_yaw_loss,\n",
    "        'roll_mae': avg_roll_loss,\n",
    "        'pitch_mae': avg_pitch_loss\n",
    "    }\n",
    "\n",
    "    # Evaluation on validation set with tqdm progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_yaw_loss = 0.0\n",
    "    val_roll_loss = 0.0\n",
    "    val_pitch_loss = 0.0\n",
    "    val_rmse_loss = 0.0\n",
    "    val_r2_score = 0.0\n",
    "    num_val_batches = len(val_loader)\n",
    "    \n",
    "    val_loader_tqdm = tqdm(val_loader, desc=\"Validation\", unit=\"batch\", mininterval=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_tqdm:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute L1 loss for yaw, roll, and pitch\n",
    "            yaw_loss = l1_loss(outputs[:, 0], targets[:, 0])\n",
    "            roll_loss = l1_loss(outputs[:, 1], targets[:, 1])\n",
    "            pitch_loss = l1_loss(outputs[:, 2], targets[:, 2])\n",
    "            avg_loss = (yaw_loss + roll_loss + pitch_loss) / 3\n",
    "            \n",
    "            # Compute RMSE and R2 loss\n",
    "            rmse = rmse_loss(outputs, targets)\n",
    "            r2 = r2_score(outputs, targets)\n",
    "            \n",
    "            val_loss += avg_loss.item()\n",
    "            val_yaw_loss += yaw_loss.item()\n",
    "            val_roll_loss += roll_loss.item()\n",
    "            val_pitch_loss += pitch_loss.item()\n",
    "            val_rmse_loss += rmse.item()\n",
    "            val_r2_score += r2 #.item()\n",
    "            \n",
    "            val_loader_tqdm.set_postfix(loss=val_loss / (len(val_loader_tqdm) + 1))\n",
    "\n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    avg_val_yaw_loss = val_yaw_loss / num_val_batches\n",
    "    avg_val_roll_loss = val_roll_loss / num_val_batches\n",
    "    avg_val_pitch_loss = val_pitch_loss / num_val_batches\n",
    "    avg_val_rmse_loss = val_rmse_loss / num_val_batches\n",
    "    avg_val_r2_score = val_r2_score / num_val_batches\n",
    "    \n",
    "    print(f'Validation Avg Loss: {avg_val_loss:.4f}, Yaw Loss: {avg_val_yaw_loss:.4f}, Roll Loss: {avg_val_roll_loss:.4f}, Pitch Loss: {avg_val_pitch_loss:.4f}, RMSE: {avg_val_rmse_loss:.4f}, R2: {avg_val_r2_score:.4f}')\n",
    "    \n",
    "    val_all_stat_dict[epoch + 1] = {\n",
    "        'lr': lr,\n",
    "        'avg_mae': avg_val_loss,\n",
    "        'yaw_mae': avg_val_yaw_loss,\n",
    "        'roll_mae': avg_val_roll_loss,\n",
    "        'pitch_mae': avg_val_pitch_loss,\n",
    "        'rmse': avg_val_rmse_loss,\n",
    "        'r2_score': avg_val_r2_score\n",
    "    }\n",
    "    \n",
    "    # Save model if validation loss has improved\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'avg_mae': avg_val_loss,\n",
    "            'yaw_mae': avg_val_yaw_loss,\n",
    "            'roll_mae': avg_val_roll_loss,\n",
    "            'pitch_mae': avg_val_pitch_loss,\n",
    "            'rmse': avg_val_rmse_loss,\n",
    "            'r2_score': avg_val_r2_score,\n",
    "        }\n",
    "        best_val_loss = avg_val_loss\n",
    "        \n",
    "        loss = best_val_loss\n",
    "        loss = str(round(loss,8)).replace('.', '_')\n",
    "        \n",
    "        best_checkpoint_name = f'effinet_checkpoint_at_epoch_{epoch}_loss_{loss}'\n",
    "        torch.save(checkpoint, f'{output_path}/{best_checkpoint_name}.pth')\n",
    "        \n",
    "        early_stop_counter = 0\n",
    "        successive_lr_update_count = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    # Early stopping and learning rate adjustment\n",
    "    if early_stop_counter >= patience:\n",
    "        \n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr'] * 0.1\n",
    "            g['lr'] = lr\n",
    "        successive_lr_update_count += 1\n",
    "        print(f'Validation loss did not improve for {patience} epochs. Reducing learning rate to {lr} and loading best model.')\n",
    "        if best_checkpoint_name == '':\n",
    "            model.load_state_dict(torch.load(prev_checkpoint_path)['model_state_dict'])\n",
    "        else:\n",
    "            checkpoint = torch.load(f'{output_path}/{best_checkpoint_name}.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        early_stop_counter = 0\n",
    "    \n",
    "    torch.save(train_all_stat_dict, f'{output_path}/effinet_train_all_stat_dict.pth')\n",
    "    torch.save(val_all_stat_dict, f'{output_path}/effinet_val_all_stat_dict.pth')\n",
    "\n",
    "    if successive_lr_update_count > max_successive_lr_update_count:\n",
    "        break\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54518f",
   "metadata": {},
   "source": [
    "## Fine-tuning of head pose to eyegaze\n",
    "\n",
    "Fine-tuning of the eye gaze from head pose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88871a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:55:39.846037Z",
     "iopub.status.busy": "2024-07-21T07:55:39.845697Z",
     "iopub.status.idle": "2024-07-21T07:55:40.487892Z",
     "shell.execute_reply": "2024-07-21T07:55:40.487067Z",
     "shell.execute_reply.started": "2024-07-21T07:55:39.846009Z"
    }
   },
   "outputs": [],
   "source": [
    "model = InceptionResNet(num_classes=2)\n",
    "checkpoint = \"/kaggle/working/eyegaze_checkpoint_at_epoch_8_loss_0_34554856.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint)['model_state_dict'])\n",
    "# model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a3aa5",
   "metadata": {},
   "source": [
    "# EYEGAZE TRAINING\n",
    "\n",
    "Eye gaze training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4318cd98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T07:55:42.943020Z",
     "iopub.status.busy": "2024-07-21T07:55:42.942669Z",
     "iopub.status.idle": "2024-07-21T11:18:14.484570Z",
     "shell.execute_reply": "2024-07-21T11:18:14.483581Z",
     "shell.execute_reply.started": "2024-07-21T07:55:42.942992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BEST VAL LOSS IS:  0.34554855867923245\n",
      "INITIAL LEARNING RATE:  0.0001\n",
      "INITIAL TRAIN STATS:  9\n",
      "INITIAL VAL STATS:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 3336/3336 [25:11<00:00,  2.21batch/s, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Avg Loss: 0.4035, X Loss: 0.4185, Y Loss: 0.3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [06:28<00:00,  2.45batch/s, loss=0.339] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3393, X Loss: 0.3536, Y Loss: 0.3250, RMSE: 0.4448, R2: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 3336/3336 [19:19<00:00,  2.88batch/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Avg Loss: 0.3982, X Loss: 0.4139, Y Loss: 0.3824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [03:22<00:00,  4.70batch/s, loss=0.348] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3481, X Loss: 0.3545, Y Loss: 0.3417, RMSE: 0.4578, R2: 0.9957\n",
      "Validation loss did not improve for 1 epochs. Reducing learning rate to 1e-05 and loading best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 3336/3336 [19:45<00:00,  2.81batch/s, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Avg Loss: 0.3845, X Loss: 0.3991, Y Loss: 0.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [03:34<00:00,  4.44batch/s, loss=0.328] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3285, X Loss: 0.3368, Y Loss: 0.3201, RMSE: 0.4338, R2: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 3336/3336 [20:12<00:00,  2.75batch/s, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Avg Loss: 0.3798, X Loss: 0.3945, Y Loss: 0.3652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [03:52<00:00,  4.10batch/s, loss=0.322] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3220, X Loss: 0.3388, Y Loss: 0.3052, RMSE: 0.4248, R2: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 3336/3336 [21:49<00:00,  2.55batch/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Avg Loss: 0.3793, X Loss: 0.3924, Y Loss: 0.3661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [04:03<00:00,  3.91batch/s, loss=0.319] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3190, X Loss: 0.3347, Y Loss: 0.3034, RMSE: 0.4216, R2: 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 3336/3336 [20:31<00:00,  2.71batch/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Avg Loss: 0.3776, X Loss: 0.3913, Y Loss: 0.3639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [03:43<00:00,  4.26batch/s, loss=0.322] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3219, X Loss: 0.3354, Y Loss: 0.3084, RMSE: 0.4232, R2: 0.9964\n",
      "Validation loss did not improve for 1 epochs. Reducing learning rate to 1.0000000000000002e-06 and loading best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 3336/3336 [20:37<00:00,  2.70batch/s, loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Avg Loss: 0.3769, X Loss: 0.3904, Y Loss: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [03:41<00:00,  4.30batch/s, loss=0.327] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3278, X Loss: 0.3486, Y Loss: 0.3070, RMSE: 0.4315, R2: 0.9962\n",
      "Validation loss did not improve for 1 epochs. Reducing learning rate to 1.0000000000000002e-07 and loading best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 3336/3336 [20:52<00:00,  2.66batch/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Avg Loss: 0.3765, X Loss: 0.3911, Y Loss: 0.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 953/953 [05:21<00:00,  2.97batch/s, loss=0.327] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.3272, X Loss: 0.3478, Y Loss: 0.3066, RMSE: 0.4315, R2: 0.9962\n",
      "Validation loss did not improve for 1 epochs. Reducing learning rate to 1.0000000000000004e-08 and loading best model.\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.L1Loss()  # Mean Absolute Error Loss\n",
    "initial_lr = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# Define the training loop\n",
    "num_epochs = 50\n",
    "starting_epoch = 9\n",
    "patience = 1\n",
    "if resume_training:\n",
    "    best_val_loss = val_stats[max(val_stats.keys())]['avg_mae']\n",
    "else:\n",
    "    best_val_loss = float('inf')\n",
    "print(\"CURRENT BEST VAL LOSS IS: \", best_val_loss)\n",
    "    \n",
    "early_stop_counter = 0\n",
    "successive_lr_update_count = 0\n",
    "max_successive_lr_update_count = 2\n",
    "\n",
    "best_checkpoint_name = ''\n",
    "lr = initial_lr\n",
    "print(\"INITIAL LEARNING RATE: \", lr)\n",
    "\n",
    "train_all_stat_dict = train_stats\n",
    "val_all_stat_dict = val_stats\n",
    "print(\"INITIAL TRAIN STATS: \", len(train_all_stat_dict))\n",
    "print(\"INITIAL VAL STATS: \", len(val_all_stat_dict))\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs!')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "for epoch in range(starting_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_x_loss = 0.0\n",
    "    running_y_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", mininterval=1)\n",
    "\n",
    "    for inputs, targets in train_loader_tqdm:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute L1 loss for yaw, roll, and pitch\n",
    "        x_loss = l1_loss(outputs[:, 0], targets[:, 0])\n",
    "        y_loss = l1_loss(outputs[:, 1], targets[:, 1])\n",
    "        avg_loss = (x_loss + y_loss) / 2\n",
    "        \n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += avg_loss.item()\n",
    "        running_x_loss += x_loss.item()\n",
    "        running_y_loss += y_loss.item()\n",
    "        \n",
    "        train_loader_tqdm.set_postfix(loss=running_loss / (len(train_loader_tqdm) + 1))\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_x_loss = running_x_loss / num_batches\n",
    "    avg_y_loss = running_y_loss / num_batches\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, X Loss: {avg_x_loss:.4f}, Y Loss: {avg_y_loss:.4f}')\n",
    "    \n",
    "\n",
    "    train_all_stat_dict[epoch + 1] = {\n",
    "        'lr': lr,\n",
    "        'mae': avg_loss,\n",
    "        'x_mae': avg_x_loss,\n",
    "        'y_mae': avg_y_loss\n",
    "    }\n",
    "\n",
    "    # Evaluation on validation set with tqdm progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_x_loss = 0.0\n",
    "    val_y_loss = 0.0\n",
    "    val_rmse_loss = 0.0\n",
    "    val_r2_score = 0.0\n",
    "    num_val_batches = len(val_loader)\n",
    "    \n",
    "    val_loader_tqdm = tqdm(val_loader, desc=\"Validation\", unit=\"batch\", mininterval=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_tqdm:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute L1 loss for yaw, roll, and pitch\n",
    "            x_loss = l1_loss(outputs[:, 0], targets[:, 0])\n",
    "            y_loss = l1_loss(outputs[:, 1], targets[:, 1])\n",
    "            avg_loss = (x_loss + y_loss) / 2\n",
    "            \n",
    "            # Compute RMSE and R2 loss\n",
    "            rmse = rmse_loss(outputs, targets)\n",
    "            r2 = r2_score(outputs, targets)\n",
    "            \n",
    "            val_loss += avg_loss.item()\n",
    "            val_x_loss += x_loss.item()\n",
    "            val_y_loss += y_loss.item()\n",
    "            val_rmse_loss += rmse.item()\n",
    "            val_r2_score += r2 #.item()\n",
    "            \n",
    "            val_loader_tqdm.set_postfix(loss=val_loss / (len(val_loader_tqdm) + 1))\n",
    "\n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    avg_val_x_loss = val_x_loss / num_val_batches\n",
    "    avg_val_y_loss = val_y_loss / num_val_batches\n",
    "    avg_val_rmse_loss = val_rmse_loss / num_val_batches\n",
    "    avg_val_r2_score = val_r2_score / num_val_batches\n",
    "    \n",
    "    print(f'Validation Avg Loss: {avg_val_loss:.4f}, X Loss: {avg_val_x_loss:.4f}, Y Loss: {avg_val_y_loss:.4f}, RMSE: {avg_val_rmse_loss:.4f}, R2: {avg_val_r2_score:.4f}')\n",
    "    \n",
    "    val_all_stat_dict[epoch + 1] = {\n",
    "        'lr': lr,\n",
    "        'avg_mae': avg_val_loss,\n",
    "        'x_mae': avg_val_x_loss,\n",
    "        'y_mae': avg_val_y_loss,\n",
    "        'rmse': avg_val_rmse_loss,\n",
    "        'r2_score': avg_val_r2_score\n",
    "    }\n",
    "    \n",
    "    # Save model if validation loss has improved\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'avg_mae': avg_val_loss,\n",
    "            'x_mae': avg_val_x_loss,\n",
    "            'y_mae': avg_val_y_loss,\n",
    "            'rmse': avg_val_rmse_loss,\n",
    "            'r2_score': avg_val_r2_score,\n",
    "        }\n",
    "        best_val_loss = avg_val_loss\n",
    "        \n",
    "        loss = best_val_loss\n",
    "        loss = str(round(loss,8)).replace('.', '_')\n",
    "        \n",
    "        best_checkpoint_name = f'eyegaze_checkpoint_at_epoch_{epoch}_loss_{loss}'\n",
    "        torch.save(checkpoint, f'{output_path}/{best_checkpoint_name}.pth')\n",
    "        \n",
    "        early_stop_counter = 0\n",
    "        successive_lr_update_count = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    # Early stopping and learning rate adjustment\n",
    "    if early_stop_counter >= patience:\n",
    "        \n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] * 0.1\n",
    "            lr = g['lr']\n",
    "            successive_lr_update_count += 1\n",
    "        print(f'Validation loss did not improve for {patience} epochs. Reducing learning rate to {lr} and loading best model.')\n",
    "        if best_checkpoint_name == '':\n",
    "            model.load_state_dict(torch.load(prev_checkpoint_path)['model_state_dict'])\n",
    "        else:\n",
    "            checkpoint = torch.load(f'{output_path}/{best_checkpoint_name}.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        early_stop_counter = 0\n",
    "    \n",
    "    torch.save(train_all_stat_dict, f'{output_path}/train_all_stat_dict.pth')\n",
    "    torch.save(val_all_stat_dict, f'{output_path}/val_all_stat_dict.pth')\n",
    "    \n",
    "    if successive_lr_update_count > max_successive_lr_update_count:\n",
    "        break\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43ed2f",
   "metadata": {},
   "source": [
    "# HEAD POSE TEST\n",
    "\n",
    "Testing code for head pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/kaggle/working/eyegaze_checkpoint_at_epoch_13_loss_0_31901939.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_yaw_loss = 0.0\n",
    "test_roll_loss = 0.0\n",
    "test_pitch_loss = 0.0\n",
    "test_rmse_loss = 0.0\n",
    "test_r2_score = 0.0\n",
    "num_test_batches = len(test_loader)\n",
    "\n",
    "test_loader_tqdm = tqdm(test_loader, desc=\"TEST\", unit=\"batch\", mininterval=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_tqdm:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute L1 loss for yaw, roll, and pitch\n",
    "        yaw_loss = l1_loss(outputs[:, 0], targets[:, 0])\n",
    "        roll_loss = l1_loss(outputs[:, 1], targets[:, 1])\n",
    "        pitch_loss = l1_loss(outputs[:, 2], targets[:, 2])\n",
    "        avg_loss = (yaw_loss + roll_loss + pitch_loss) / 3\n",
    "\n",
    "        # Compute RMSE and R2 loss\n",
    "        rmse = rmse_loss(outputs, targets)\n",
    "        r2 = r2_score(outputs, targets)\n",
    "\n",
    "        test_loss += avg_loss.item()\n",
    "        test_yaw_loss += yaw_loss.item()\n",
    "        test_roll_loss += roll_loss.item()\n",
    "        test_pitch_loss += pitch_loss.item()\n",
    "        test_rmse_loss += rmse.item()\n",
    "        test_r2_score += r2 #.item()\n",
    "\n",
    "        test_loader_tqdm.set_postfix(loss=test_loss / (len(test_loader_tqdm) + 1))\n",
    "\n",
    "avg_test_loss = test_loss / num_test_batches\n",
    "avg_test_yaw_loss = test_yaw_loss / num_test_batches\n",
    "avg_test_roll_loss = test_roll_loss / num_test_batches\n",
    "avg_test_pitch_loss = test_pitch_loss / num_test_batches\n",
    "avg_test_rmse_loss = test_rmse_loss / num_test_batches\n",
    "avg_test_r2_score = test_r2_score / num_test_batches\n",
    "\n",
    "print(f'Validation Avg Loss: {avg_test_loss:.4f}, Yaw Loss: {avg_test_yaw_loss:.4f}, Roll Loss: {avg_test_roll_loss:.4f}, Pitch Loss: {avg_test_pitch_loss:.4f}, RMSE: {avg_test_rmse_loss:.4f}, R2: {avg_test_r2_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da14f799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:44:45.448313Z",
     "iopub.status.busy": "2024-07-21T13:44:45.447948Z",
     "iopub.status.idle": "2024-07-21T13:44:46.060290Z",
     "shell.execute_reply": "2024-07-21T13:44:46.059429Z",
     "shell.execute_reply.started": "2024-07-21T13:44:45.448284Z"
    }
   },
   "outputs": [],
   "source": [
    "class EfficientNetV2Regression(nn.Module):\n",
    "    def __init__(self, num_outputs=2):\n",
    "        super(EfficientNetV2Regression, self).__init__()\n",
    "        self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "       \n",
    "        # Replace the classifier layer to output `num_outputs` regression targets\n",
    "        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, num_outputs)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetV2Regression(num_outputs=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af6cc574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:30:00.710853Z",
     "iopub.status.busy": "2024-07-21T13:30:00.709952Z",
     "iopub.status.idle": "2024-07-21T13:30:01.716785Z",
     "shell.execute_reply": "2024-07-21T13:30:01.715814Z",
     "shell.execute_reply.started": "2024-07-21T13:30:00.710809Z"
    }
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "model = timm.create_model('inception_resnet_v2', pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "in_features = model.classif.in_features\n",
    "model.classif = nn.Linear(in_features, 2)\n",
    "model.classif = model.classif.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57060223",
   "metadata": {},
   "source": [
    "# EYEGAZE TEST CODE\n",
    "\n",
    "Testing code for eye gaze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ab6b833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:48:11.104960Z",
     "iopub.status.busy": "2024-07-21T13:48:11.104183Z",
     "iopub.status.idle": "2024-07-21T13:50:09.979085Z",
     "shell.execute_reply": "2024-07-21T13:50:09.978085Z",
     "shell.execute_reply.started": "2024-07-21T13:48:11.104926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST: 100%|██████████| 477/477 [01:58<00:00,  4.03batch/s, loss=0.256] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Avg Loss: 0.2567, x Loss: 0.2893, y Loss: 0.2242, RMSE: 0.3406, R2: 0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"/kaggle/input/eyegaze-effinet-model-and-data/eyegaze_checkpoint_at_epoch_7_loss_0_27638153.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_x_loss = 0.0\n",
    "test_y_loss = 0.0\n",
    "test_rmse_loss = 0.0\n",
    "test_r2_score = 0.0\n",
    "num_test_batches = len(test_loader)\n",
    "\n",
    "test_loader_tqdm = tqdm(test_loader, desc=\"TEST\", unit=\"batch\", mininterval=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_tqdm:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute L1 loss for x, y, and pitch\n",
    "        x_loss = l1_loss(outputs[:, 0], targets[:, 0])\n",
    "        y_loss = l1_loss(outputs[:, 1], targets[:, 1])\n",
    "        avg_loss = (x_loss + y_loss) / 2\n",
    "\n",
    "        # Compute RMSE and R2 loss\n",
    "        rmse = rmse_loss(outputs, targets)\n",
    "        r2 = r2_score(outputs, targets)\n",
    "\n",
    "        test_loss += avg_loss.item()\n",
    "        test_x_loss += x_loss.item()\n",
    "        test_y_loss += y_loss.item()\n",
    "        test_rmse_loss += rmse.item()\n",
    "        test_r2_score += r2 #.item()\n",
    "\n",
    "        test_loader_tqdm.set_postfix(loss=test_loss / (len(test_loader_tqdm) + 1))\n",
    "\n",
    "avg_test_loss = test_loss / num_test_batches\n",
    "avg_test_x_loss = test_x_loss / num_test_batches\n",
    "avg_test_y_loss = test_y_loss / num_test_batches\n",
    "avg_test_rmse_loss = test_rmse_loss / num_test_batches\n",
    "avg_test_r2_score = test_r2_score / num_test_batches\n",
    "\n",
    "print(f'Validation Avg Loss: {avg_test_loss:.4f}, x Loss: {avg_test_x_loss:.4f}, y Loss: {avg_test_y_loss:.4f}, RMSE: {avg_test_rmse_loss:.4f}, R2: {avg_test_r2_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed3a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5289677,
     "sourceId": 8796921,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5367960,
     "sourceId": 8924279,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5368211,
     "sourceId": 8924616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5368297,
     "sourceId": 8924730,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5368350,
     "sourceId": 8924795,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5421237,
     "sourceId": 8999662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5421240,
     "sourceId": 8999666,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5421254,
     "sourceId": 8999689,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5421260,
     "sourceId": 8999700,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5423626,
     "sourceId": 9003059,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5423794,
     "sourceId": 9003292,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5423835,
     "sourceId": 9003354,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
