{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1b7e38-6f33-471f-81ef-e8e67a17e76c",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "1. Make sure your computer webcam works properly. \n",
    "2. Run all code cells below sequentially.\n",
    "3. Run the camera runner cell at the end to open the camera and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121cf3bf-131d-4ba2-be8d-c983e0a60746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import Label\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5e9e6-91ca-42d6-b80b-7631f4cc5bd3",
   "metadata": {},
   "source": [
    "Check the device (CPU or GPU) and set it to variable device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f765e9cb-818e-40de-856b-a95ce9b06b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9675f3-0310-4b69-b026-a4edde061436",
   "metadata": {},
   "source": [
    "Seed randomness for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b84516-643f-464f-907b-42cdc0e72136",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb460e79-d866-4079-87d2-055c63319394",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa67b4-187c-4fc6-ac60-f95c8a6af404",
   "metadata": {},
   "source": [
    "## Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce224f-4031-434f-87f6-27e274738e15",
   "metadata": {},
   "source": [
    "This is the custom model code. If you would like to run the camera code with custom model, run this code and skip the next two code cells, which belong to transfer models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ecab47e-d748-4d7f-a586-cc897a8cd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride=1):\n",
    "        super(FactorizedConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(1, kernel_size), stride=stride, padding=(0, padding), bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(kernel_size, 1), stride=stride, padding=(padding, 0), bias=True)\n",
    "      \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.ReLU(inplace=True)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return nn.ReLU(inplace=True)(x)\n",
    "class InceptionResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, scale=1.0):\n",
    "        super(InceptionResNetBlock, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            FactorizedConv2d(32, 48, kernel_size=3, padding=1),\n",
    "            FactorizedConv2d(48, 64, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            FactorizedConv2d(32, 48, kernel_size=3, padding=1),\n",
    "            FactorizedConv2d(48, 64, kernel_size=7, padding=3),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            FactorizedConv2d(32, 48, kernel_size=5, padding=2),\n",
    "            FactorizedConv2d(48, 64, kernel_size=7, padding=3),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, bias=True)\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv2d(128, in_channels, kernel_size=1, bias=True)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch0 = self.branch0(x)\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "\n",
    "        mixed = torch.cat([branch0, branch1, branch2, branch3], dim=1)\n",
    "        up = self.conv(mixed)\n",
    "        up = self.bn(up)\n",
    "\n",
    "        x = x + self.scale * up\n",
    "        return nn.ReLU(inplace=True)(x)\n",
    "\n",
    "class InceptionResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(InceptionResNet, self).__init__()\n",
    "        self.model = None\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(32, 32, kernel_size=3, padding=0, bias=True),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "            nn.Conv2d(64, 80, kernel_size=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(80),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(80, 192, kernel_size=3, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.inception_resnet_a = nn.Sequential(\n",
    "            InceptionResNetBlock(192, scale=0.2),\n",
    "            InceptionResNetBlock(192, scale=0.2),\n",
    "            # InceptionResNetBlock(192, scale=0.2),\n",
    "            # InceptionResNetBlock(192, scale=0.2)\n",
    "        )\n",
    "\n",
    "        self.reduction_a = nn.Sequential(\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inception_resnet_b = nn.Sequential(\n",
    "            InceptionResNetBlock(384, scale=0.2),\n",
    "            InceptionResNetBlock(384, scale=0.2),\n",
    "            # InceptionResNetBlock(384, scale=0.2),\n",
    "            # InceptionResNetBlock(384, scale=0.2)\n",
    "        )\n",
    "\n",
    "        self.reduction_b = nn.Sequential(\n",
    "            nn.Conv2d(384, 1024, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inception_resnet_c = nn.Sequential(\n",
    "            InceptionResNetBlock(1024, scale=0.2),\n",
    "            InceptionResNetBlock(1024, scale=0.2),\n",
    "            # InceptionResNetBlock(1024, scale=0.2),\n",
    "            # InceptionResNetBlock(1024, scale=0.2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception_resnet_a(x)\n",
    "        x = self.reduction_a(x)\n",
    "        x = self.inception_resnet_b(x)\n",
    "        x = self.reduction_b(x)\n",
    "        x = self.inception_resnet_c(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def get_model(self):\n",
    "        # Define the model, loss function, and optimizer\n",
    "        self.model = InceptionResNet()\n",
    "        self.model = self.model.to(device)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc801e0-0eec-4df5-8e97-fb2fd109ac07",
   "metadata": {},
   "source": [
    "## Efficient Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88d29f-7372-441d-8ade-5c899c4e85cd",
   "metadata": {},
   "source": [
    "This is a code for EfficientNetv2 transfer learning model. It downloads the pretrained model from torchvision for fine-tuining. If you would like to run the camera code with EfficientNet model, run this code cell and skip the next and above code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c2026a-1cbc-4c14-8058-1614832fdf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Regression(nn.Module):\n",
    "    def __init__(self, num_outputs=3):\n",
    "        super(EfficientNetV2Regression, self).__init__()\n",
    "        self.model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "       \n",
    "        # Replace the classifier layer to output `num_outputs` regression targets\n",
    "        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, num_outputs)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def get_model(self, channels):\n",
    "        # Instantiate the model\n",
    "        # model = EfficientNetV2Regression(num_outputs=3)\n",
    "        # model = model.to(device)\n",
    "        return self.model(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbaf8f7-55b8-40a0-ab9b-01c991385ba2",
   "metadata": {},
   "source": [
    "## Inception ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01cf2f-8696-4cf0-aadb-95c5b762a27f",
   "metadata": {},
   "source": [
    "This is a code for Inception-ResNet transfer learning model. It downloads the pretrained model with timm library for fine-tuining. If you would like to run the camera code with Inception-ResNet model, run this code cell and skip the above two code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacb83b8-dbe7-4789-8449-95491aebb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "def incepResNet(out_channels):\n",
    "    model = timm.create_model('inception_resnet_v2', pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    in_features = model.classif.in_features\n",
    "    model.classif = nn.Linear(in_features, out_channels)\n",
    "    model.classif = model.classif.to(device)\n",
    "    \n",
    "    # model.load_state_dict(torch.load(\n",
    "    #     \"../../HEAD_POSE/FINE_TUNE/TRANSFER/IncepResNetv2/ALL_DATA/head_pose_checkpoint_at_epoch_24_loss_1_79185119.pth\", \n",
    "    #                                  map_location=device)['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5dca9-5a8c-4a56-b69e-9432502d79ef",
   "metadata": {},
   "source": [
    "The following code cell sets up the model to be used with the camera application. It loads the saved checkpoints to the models and return the model. Change the path to the checkpoint in your file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09fff79b-f313-4ea6-84da-abba379f1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(out_channels = 3, type=\"\"):\n",
    "    model = None\n",
    "    \n",
    "    path_to_headpose_checkpoint = \"path/to/head pose/model/checkpoint\" # Change the path to your checkpoints path\n",
    "    path_to_eyegaze_checkpoint = \"path/to/eye gaze/model/checkpoint\" # Change the path to your checkpoints path\n",
    "\n",
    "    if type == \"custom\":\n",
    "        model = InceptionResNet(out_channels)\n",
    "        if out_channels == 3:\n",
    "            model.load_state_dict(torch.load(path_to_headpose_checkpoint, \n",
    "                                             map_location=device)['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(path_to_eyegaze_checkpoint, \n",
    "                                             map_location=device)['model_state_dict'])\n",
    "            \n",
    "    elif type == \"incep\":\n",
    "        model = incepResNet(out_channels)\n",
    "        if out_channels == 3: \n",
    "            model.load_state_dict(torch.load(path_to_headpose_checkpoint, \n",
    "                                         map_location=device)['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(path_to_eyegaze_checkpoint, \n",
    "                                         map_location=device)['model_state_dict'])\n",
    "            \n",
    "    elif type == \"effinet\":\n",
    "        model = EfficientNetV2Regression(out_channels)\n",
    "        if out_channels == 3: \n",
    "            model.load_state_dict(torch.load(path_to_headpose_checkpoint, \n",
    "                                         map_location=device)['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(path_to_eyegaze_checkpoint, \n",
    "                                         map_location=device)['model_state_dict'])\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f9aa4-2599-49d7-a11b-be42e2cadb30",
   "metadata": {},
   "source": [
    "# Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262bbd6-82d1-4166-8a34-463e05cc7d26",
   "metadata": {},
   "source": [
    "This is the camera main code. It holds the a class with all the functions to capture the frame, preprocess the input image, predict the head pose and eye gaze, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcb15367-f57f-4ded-8def-441b4a426bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import Label\n",
    "from PIL import Image, ImageTk\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CameraApp:\n",
    "    def __init__(self, window, window_title, model, gaze_model, head_threshold, eye_threshold):\n",
    "        self.window = window\n",
    "        self.window.title(window_title)\n",
    "        self.video_source = 0  # Default camera\n",
    "        self.model = model\n",
    "        # self.model.eval()  # Set model to evaluation mode\n",
    "        self.gaze_model = gaze_model\n",
    "        # self.gaze_model.eval()  # Set gaze model to evaluation mode\n",
    "\n",
    "        self.head_threshold = head_threshold\n",
    "        self.eye_threshold = eye_threshold\n",
    "\n",
    "        # Load Haar Cascade for face and eye detection\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "        if self.face_cascade.empty() or self.eye_cascade.empty():\n",
    "            raise IOError(\"Failed to load one or more cascade classifiers. Check the paths to the XML files.\")\n",
    "\n",
    "        # Open the video source\n",
    "        self.vid = cv2.VideoCapture(self.video_source)\n",
    "        if not self.vid.isOpened():\n",
    "            raise ValueError(\"Unable to open video source\", self.video_source)\n",
    "\n",
    "        # Create a frame to hold the video feed and buttons\n",
    "        self.video_frame = tk.Frame(window)\n",
    "        self.video_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Create a canvas that can fit the video source size\n",
    "        self.canvas = tk.Canvas(self.video_frame, width=self.vid.get(cv2.CAP_PROP_FRAME_WIDTH), height=self.vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.canvas.pack()\n",
    "\n",
    "        # Button to snapshot\n",
    "        self.btn_snapshot = tk.Button(self.video_frame, text=\"Capture\", width=50, command=self.snapshot)\n",
    "        self.btn_snapshot.pack(anchor=tk.CENTER, expand=True)\n",
    "\n",
    "        # Button to close\n",
    "        self.btn_close = tk.Button(self.video_frame, text=\"Close\", width=50, command=self.on_closing)\n",
    "        self.btn_close.pack(anchor=tk.CENTER, expand=True)\n",
    "\n",
    "\n",
    "        # Create a frame to hold the snapshot and status message\n",
    "        self.right_frame = tk.Frame(window)\n",
    "        self.right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Snapshot display areas\n",
    "        self.snapshot_area = tk.Label(self.right_frame)\n",
    "        self.snapshot_area.pack(side=tk.TOP, expand=True)\n",
    "\n",
    "        self.face_snapshot_area = tk.Label(self.right_frame)\n",
    "        self.face_snapshot_area.pack(side=tk.TOP, expand=True)\n",
    "\n",
    "        self.eye_snapshot_area = tk.Label(self.right_frame)\n",
    "        self.eye_snapshot_area.pack(side=tk.TOP, expand=True)\n",
    "\n",
    "        # Label to display status messages\n",
    "        self.status_label = tk.Label(self.right_frame, text=\"Ready\", font=(\"Helvetica\", 12))\n",
    "        self.status_label.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "\n",
    "        self.head_pose_threshold = tk.Label(self.right_frame)\n",
    "        self.head_pose_threshold.pack(pady=5)\n",
    "        self.head_pose_threshold.config(text=f\"Head Pose Threshold: {self.head_threshold}\")\n",
    "\n",
    "        self.eye_gaze_threshold = tk.Label(self.right_frame)\n",
    "        self.eye_gaze_threshold.pack(pady=5)\n",
    "        self.eye_gaze_threshold.config(text=f\"Eye-Gaze Threshold: {self.eye_threshold}\")\n",
    "        \n",
    "        self.head_pose_pred = tk.Label(self.right_frame, text=\"Head Pose Angles Prediction: \")\n",
    "        self.head_pose_pred.pack(pady=5)\n",
    "        \n",
    "        self.eye_gaze_pred = tk.Label(self.right_frame, text=\"Eye-gaze Predictions: \")\n",
    "        self.eye_gaze_pred.pack(pady=5)\n",
    "\n",
    "        \n",
    "\n",
    "        # After it is called once, the update method will be automatically called every delay milliseconds\n",
    "        self.delay = 15\n",
    "\n",
    "        # Store the latest image to avoid garbage collection\n",
    "        self.photo = None\n",
    "\n",
    "        self.update()\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "        self.window.mainloop()\n",
    "\n",
    "    def snapshot(self):\n",
    "        # Get a frame from the video source\n",
    "        ret, frame = self.vid.read()\n",
    "\n",
    "        if ret:\n",
    "            # Flip the frame horizontally for mirror effect\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Add a green border to the frame\n",
    "            border_color_green = (0, 255, 0)  # Green\n",
    "            border_color_red = (0, 0, 255)  # Red\n",
    "            border_size = 10\n",
    "\n",
    "            # Detect and crop the face region\n",
    "            face_image = self.detect_and_crop_face(frame)\n",
    "            if face_image is not None:\n",
    "                # Convert the cropped face region to PIL format and display it\n",
    "                face_img_name = \"cropped_face-frame-\" + str(int(self.vid.get(cv2.CAP_PROP_POS_FRAMES))) + \".jpg\"\n",
    "                cv2.imwrite(face_img_name, face_image)\n",
    "                face_img = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "                face_img_pil = Image.fromarray(face_img)\n",
    "\n",
    "                # Transform the image and make a prediction\n",
    "                image_tensor = self.transform_image_for_pose(face_img_pil)\n",
    "                prediction = self.predict_pose(image_tensor)\n",
    "                prediction = np.round(prediction.numpy(), 2)\n",
    "                self.head_pose_pred.config(text=f\"Predicted Head Pose: (Yaw: {prediction[0][0]:.2f}, Roll: {prediction[0][1]:.2f}, Pitch: {prediction[0][2]:.2f})\")\n",
    "                print(\"Head Pose Prediction: \", prediction)\n",
    "                head_pose_is_right = True\n",
    "\n",
    "                frame_with_border = cv2.copyMakeBorder(frame, border_size, border_size, border_size, border_size, cv2.BORDER_CONSTANT, value=border_color_green)\n",
    "                self.log(\"Photo Captured Successfully\")\n",
    "                if (prediction[0][0] < -1 * self.head_threshold or prediction[0][0] > self.head_threshold) or (prediction[0][1] < - 1 * self.head_threshold or prediction[0][1] > self.head_threshold) or (prediction[0][2] < -1 * self.head_threshold or prediction[0][2] > self.head_threshold):\n",
    "                    frame_with_border = cv2.copyMakeBorder(frame, border_size, border_size, border_size, border_size, cv2.BORDER_CONSTANT, value=border_color_red)\n",
    "                    self.log(\"Adjust Your Head Position!\".title())\n",
    "                    head_pose_is_right = False\n",
    "                # Detect and crop the eye regions\n",
    "                eye = self.detect_and_crop_eyes(face_image)\n",
    "\n",
    "                if eye is not None and head_pose_is_right:\n",
    "                    # Convert the cropped eye region to PIL format and display it\n",
    "                    eye_img = cv2.cvtColor(eye, cv2.COLOR_BGR2RGB)\n",
    "                    eye_img_pil = Image.fromarray(eye_img)\n",
    "                    # eye_img_tk = ImageTk.PhotoImage(image=eye_img_pil)\n",
    "                    # self.eye_snapshot_area.config(image=eye_img_tk)\n",
    "                    # self.eye_snapshot_area.image = eye_img_tk\n",
    "\n",
    "                    # Transform the image and make a gaze prediction\n",
    "                    gaze_tensor = self.transform_image_for_gaze(eye_img_pil)\n",
    "                    gaze_prediction = self.predict_gaze(gaze_tensor)\n",
    "\n",
    "                    gaze_prediction = np.round(gaze_prediction.numpy(), 2)\n",
    "                    self.eye_gaze_pred.config(text=f\"Eye-gaze Prediction: (X: {gaze_prediction[0][0]:.2f}, Y: {gaze_prediction[0][1]:.2f})\")\n",
    "                \n",
    "                    print(f'Eye Gaze Prediction: {gaze_prediction}')\n",
    "\n",
    "                    if (gaze_prediction[0][0] < -1 * self.eye_threshold or gaze_prediction[0][0] > self.eye_threshold) or (gaze_prediction[0][1] < -1 * self.eye_threshold or gaze_prediction[0][1] > self.eye_threshold):\n",
    "                        frame_with_border = cv2.copyMakeBorder(frame, border_size, border_size, border_size, border_size, cv2.BORDER_CONSTANT, value=border_color_red)\n",
    "                        self.log(\"Look Straight at the Camera!\".title())\n",
    "                # else:\n",
    "                #     self.status_label.config(text=f\"Status: Eyes not detected!\")\n",
    "                        \n",
    "                # Save the captured image\n",
    "                img_name = \"frame-\" + str(int(self.vid.get(cv2.CAP_PROP_POS_FRAMES))) + \".jpg\"\n",
    "                cv2.imwrite(img_name, frame_with_border)\n",
    "\n",
    "                # Convert the image to PIL format and display it on the side\n",
    "                img = cv2.cvtColor(frame_with_border, cv2.COLOR_BGR2RGB)\n",
    "                img_pil = Image.fromarray(img)\n",
    "                img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "                self.snapshot_area.config(image=img_tk)\n",
    "                self.snapshot_area.image = img_tk\n",
    "\n",
    "                # print(f'Prediction: {prediction}')\n",
    "            else:\n",
    "                print(\"Face not detected!\")\n",
    "                # Set a default image or handle the case where the face is not detected\n",
    "                self.snapshot_area.config(image=None)\n",
    "                self.snapshot_area.image = None\n",
    "\n",
    "    def detect_and_crop_face(self, image):\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect face in the image\n",
    "        faces = self.face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # Assume the first detected face is the main face\n",
    "            x, y, w, h = faces[0]\n",
    "\n",
    "            # Increase the face region in the y-dimension\n",
    "            margin_y = int(h * 0.10)  # Add 20% margin to the height\n",
    "            y1 = max(0, y - margin_y)  # Ensure y does not go below 0\n",
    "            h1 = h + margin_y  # Increase the height\n",
    "\n",
    "            # Crop the face region with extended margin\n",
    "            face_region = image[y1:y + h1, x:x + w]\n",
    "            return face_region\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def detect_and_crop_eyes(self, face_image):\n",
    "        # Convert the face image to grayscale\n",
    "        gray_face = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect eyes in the face region\n",
    "        eyes = self.eye_cascade.detectMultiScale(gray_face, scaleFactor=1.1, minNeighbors=10, minSize=(30, 30))\n",
    "\n",
    "        if len(eyes) == 2:\n",
    "            # Sort detected eyes by x coordinate to ensure left-to-right order\n",
    "            eyes = sorted(eyes, key=lambda x: x[0])\n",
    "\n",
    "            # Get the coordinates for the combined bounding box\n",
    "            x1 = min(eyes[0][0], eyes[1][0])\n",
    "            y1 = min(eyes[0][1], eyes[1][1])\n",
    "            x2 = max(eyes[0][0] + eyes[0][2], eyes[1][0] + eyes[1][2])\n",
    "            y2 = max(eyes[0][1] + eyes[0][3], eyes[1][1] + eyes[1][3])\n",
    "\n",
    "            # Add some margin to the bounding box\n",
    "            margin_y = int((y2 - y1) * 0.20)  # 20% of the height of the bounding box\n",
    "\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1 + margin_y)\n",
    "            x2 = min(face_image.shape[1], x2)\n",
    "            y2 = min(face_image.shape[0], y2 - margin_y)\n",
    "\n",
    "            # Crop the region containing both eyes with margin\n",
    "            eye_region = face_image[y1:y2, x1:x2]\n",
    "            return eye_region\n",
    "\n",
    "        return None\n",
    "\n",
    "    def transform_image_for_pose(self, image):\n",
    "        # Transform the image to tensor and normalize it\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4702, 0.3964, 0.3711), (0.2337, 0.2362, 0.2483))\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)  # Add batch dimension\n",
    "        return image\n",
    "\n",
    "    def transform_image_for_gaze(self, image):\n",
    "        # Transform the image to tensor and normalize it for gaze model\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((229, 229)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.6965, 0.5065, 0.40670), (0.2380, 0.2134, 0.1928))\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)  # Add batch dimension\n",
    "        return image\n",
    "\n",
    "    def predict_pose(self, image):\n",
    "        # Make a prediction with the pose model\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image)\n",
    "        # print(output)\n",
    "        return output\n",
    "\n",
    "    def predict_gaze(self, image):\n",
    "        # Make a prediction with the gaze model\n",
    "        self.gaze_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.gaze_model(image)\n",
    "        # print(output)\n",
    "        return output\n",
    "\n",
    "    def update(self):\n",
    "        # Get a frame from the video source\n",
    "        ret, frame = self.vid.read()\n",
    "\n",
    "        if ret:\n",
    "            # Flip the frame horizontally for mirror effect\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Convert the image to PIL format\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            self.photo = ImageTk.PhotoImage(image=pil_image)  # Store the photo as an instance variable\n",
    "            self.canvas.create_image(0, 0, image=self.photo, anchor=tk.NW)\n",
    "\n",
    "        self.window.after(self.delay, self.update)\n",
    "\n",
    "    def on_closing(self):\n",
    "        # Release the video source when the object is destroyed\n",
    "        if self.vid.isOpened():\n",
    "            self.vid.release()\n",
    "        self.window.destroy()\n",
    "\n",
    "    def log(self, message):\n",
    "        \"\"\"Update the text of the status label.\"\"\"\n",
    "        self.status_label.config(text=f\"Status: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09141d-0b71-4add-a183-9c1d61c26d1e",
   "metadata": {},
   "source": [
    "# Run the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13906745-5183-4460-ab1e-0e364f033208",
   "metadata": {},
   "source": [
    "This cell is the camera app runner. It creates objects for head pose, eye gaze models, and the camera class and sends the two models as parameters to the camera object. When you create the chosen model object from the three models, correctly set the \"type\" parameter: `custom` for the custom model, `effinet` for the EfficientNetv2 model, and `incep` for the inception-resnet model.\n",
    "\n",
    "Set the required threshold for the optimal range of both head pose and eye gaze estimation. For example, if the threshold value for head pose is 10, it means that the predicted yaw, roll, and pitch values of head pose are considered optimal when they fall between -10 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46d6a193-19ad-47f3-9963-5ad0f38270be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Pose Prediction:  [[ 0.5   1.82 22.4 ]]\n",
      "Head Pose Prediction:  [[-1.85 -3.76 12.08]]\n",
      "Head Pose Prediction:  [[ 0.21  6.45 13.42]]\n",
      "Head Pose Prediction:  [[-1.69 10.19  3.8 ]]\n"
     ]
    }
   ],
   "source": [
    "# Load your trained models (replace with your model's loading code)\n",
    "pose_model = get_model(out_channels=3, type=\"effinet\")\n",
    "# pose_model.load_state_dict(torch.load(\"pose_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "gaze_model = get_model(out_channels=2, type=\"effinet\")\n",
    "# gaze_model.load_state_dict(torch.load(\"gaze_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "head_threshold = 10.0\n",
    "eye_threshold = 8.0\n",
    "# Create a window and pass it to the Application object\n",
    "root = tk.Tk()\n",
    "app = CameraApp(root, \"MSC Project Experimental camera\", pose_model, gaze_model, head_threshold, eye_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb954bf6-b35e-45cf-a2fe-07265733643d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
